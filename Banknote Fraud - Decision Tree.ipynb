{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Banknote Fraud Detection\n",
    "\n",
    "This notebook goes over the process of using a decision tree to perform a binary classification on banknotes. These banknotes will be labeled as either fraudulent or not (class 1 and 0, respectively)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Data Description**:\n",
    "\n",
    "The data was downloaded from [this source](https://archive.ics.uci.edu/ml/datasets/banknote+authentication#). \n",
    "\n",
    "Extracted from images were taken from genuine and forged banknote-like specimens. For digitization, an industrial camera usually used for print inspection was used. The final images have 400x 400 pixels. Due to the object lens and distance to the investigated object gray-scale pictures with a resolution of about 660 dpi were gained. Wavelet Transform tool were used to extract features from images.\n",
    "\n",
    "**Attribute Information**:\n",
    "1. variance of Wavelet Transformed image (continuous)\n",
    "2. skewness of Wavelet Transformed image (continuous)\n",
    "3. curtosis of Wavelet Transformed image (continuous)\n",
    "4. entropy of image (continuous)\n",
    "5. class (integer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data(split=0.8, shuffle=False):    \n",
    "    data = pd.read_csv(\n",
    "        'banknote_data.txt',\n",
    "        sep=\",\",\n",
    "        header=None,\n",
    "        names=[\"x0\", \"x1\", \"x2\", \"x3\", \"y\"])\n",
    "    # Split data into 80-20 train-test split\n",
    "    train_size = round(split * len(data))\n",
    "    if shuffle == True:\n",
    "        row_number = data.index.tolist()\n",
    "        train_rows = random.sample(\n",
    "            population=row_number, \n",
    "            k=train_size)\n",
    "        train_data = data.loc[train_rows]\n",
    "        test_data = data.drop(train_rows)\n",
    "    else:\n",
    "        train_data, test_data = np.split(data, [train_size])\n",
    "        print(\"data imported\")\n",
    "    X_train = train_data[['x0', 'x1', 'x2', 'x3']].to_numpy()\n",
    "    y_train = train_data[['y']].to_numpy().flatten()\n",
    "    X_test = test_data[['x0', 'x1', 'x2', 'x3']].to_numpy()\n",
    "    y_test = test_data[['y']].to_numpy().flatten()\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"Each node of our decision tree will hold values such as left\n",
    "    and right children, the data and labels being split on, the \n",
    "    threshold value & index in the dataframe for a particular \n",
    "    feature, and the uncertainty measure for this node\"\"\"\n",
    "    def __init__(self, data, labels, depth):\n",
    "        \"\"\"\n",
    "        data: X data\n",
    "        labels: y data\n",
    "        depth: depth of tree\n",
    "        \"\"\"\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.depth = depth\n",
    "\n",
    "        self.threshold = None # threshold value\n",
    "        self.threshold_index = None # threshold index\n",
    "        self.feature = None # feature as a NUMBER (column number)\n",
    "        self.label = None # y label\n",
    "        self.uncertainty = None # uncertainty value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Entropy and Gini Calculations**:\n",
    "$X=\\{ x_i\\}_i^n$ and $\\sum\\limits_i P(x_i) = 1$ $\\implies$\n",
    "$$\\text{Entropy, }\\;\\; E(X) = \\sum\\limits_{i=1}^n  P(x_i) \\cdot \\log_2(P(x_i)) $$\n",
    "\n",
    "$$\\text{Total Entropy, }\\;\\; E_{tot}(X) = \\sum\\limits_{i=1}^n P(x_i)\\cdot E(X) $$\n",
    "\n",
    "$$\\text{Gini impurity, }\\;\\; G(X)=\\sum\\limits_{i=1}^nP(x_i)\\sum\\limits_{k\\neq i}P(x_k) = \\sum\\limits_{i=1}^nP(x_i)(1-P(x_i)) = 1-\\sum\\limits_{i=1}^n P(x_i)^2  $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entropy and Gini Calculations\n",
    "def entropy(y_labels):\n",
    "    _, fraud_count = np.unique(y_labels, return_counts=True)\n",
    "    p_i = fraud_count / fraud_count.sum() # probability (array) of each class\n",
    "    entropy = sum(p_i * -np.log2(p_i))\n",
    "    return entropy\n",
    "\n",
    "def gini(y_labels):\n",
    "    _, fraud_count = np.unique(y_labels, return_counts=True)\n",
    "    p_i = fraud_count / fraud_count.sum() # probability (array) of each class\n",
    "    gini = 1 - sum(p_i**2)\n",
    "    return gini\n",
    "    \n",
    "def total_entropy(partition_0, partition_1):\n",
    "    n = len(partition_0) + len(partition_1)\n",
    "    prob_part0 = len(partition_0) / n\n",
    "    prob_part1 = len(partition_1) / n\n",
    "    tot_entropy = (prob_part0 * entropy(partition_0)\n",
    "        + prob_part1 * entropy(partition_1) )\n",
    "    return tot_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, K=5, verbose=False):\n",
    "        \"\"\"\n",
    "        K: number of features to split on \n",
    "        \"\"\"\n",
    "        self.root = None\n",
    "        self.K = K\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def buildTree(self, data, labels):\n",
    "        \"\"\"Builds tree for training on data. Recursively called _buildTree\"\"\"\n",
    "        self.root = Node(data, labels, 0)\n",
    "        if self.verbose:\n",
    "            print(\"Root node shape: \", data.shape, labels.shape)\n",
    "        self._buildTree(self.root)\n",
    "\n",
    "    def _buildTree(self, node):\n",
    "        # get uncertainty measure and feature threshold\n",
    "        node.uncertainty = self.get_uncertainty(node.labels)\n",
    "        self.get_feature_threshold(node)\n",
    "        # sort feature for return\n",
    "        index = node.data[:, node.feature].argsort()\n",
    "        node.data = node.data[index]\n",
    "        node.labels = node.labels[index]      \n",
    "        \n",
    "        label_distribution = np.bincount(node.labels)       \n",
    "        if self.verbose:\n",
    "            print(\"Node uncertainty: %f\" % node.uncertainty)\n",
    "        \n",
    "        # Split left and right if threshold is not a minima of the feature \n",
    "        if (node.threshold_index == 0 or \n",
    "                node.threshold_index == node.data.shape[0] or\n",
    "                len(label_distribution) == 1):\n",
    "            node.label = (node.labels[0] if len(label_distribution) \n",
    "                    == 1 else np.argmax(label_distribution) )\n",
    "        else:\n",
    "            node.left = Node(node.data[:node.threshold_index], node.labels[:node.threshold_index], node.depth + 1)\n",
    "            node.right = Node(node.data[node.threshold_index:], node.labels[node.threshold_index:], node.depth + 1)\n",
    "            node.data = None\n",
    "            node.labels = None\n",
    "            # If in last layer of tree, assign predictions\n",
    "            if node.depth == self.K:\n",
    "                if len(node.left.labels) == 0:\n",
    "                    node.right.label = np.argmax(np.bincount(node.right.labels))\n",
    "                    node.left.label = 1 - node.right.label\n",
    "                elif len(node.right.labels) == 0:\n",
    "                    node.left.label = np.argmax(np.bincount(node.left.labels))\n",
    "                    node.right.label = 1 - node.left.label\n",
    "                else:\n",
    "                    node.left.label = np.argmax(np.bincount(node.left.labels))\n",
    "                    node.right.label = np.argmax(np.bincount(node.right.labels))\n",
    "                return\n",
    "\n",
    "            else: # Otherwise continue training the tree\n",
    "                self._buildTree(node.left)\n",
    "                self._buildTree(node.right)\n",
    "\n",
    "    def predict(self, data_pt):\n",
    "        return self._predict(data_pt, self.root)\n",
    "\n",
    "    def _predict(self, data_pt, node):\n",
    "        feature = node.feature\n",
    "        threshold = node.threshold\n",
    "        if node.label is not None:\n",
    "            return node.label\n",
    "        elif data_pt[node.feature] < node.threshold:\n",
    "            return self._predict(data_pt, node.left)\n",
    "        elif data_pt[node.feature] >= node.threshold:\n",
    "            return self._predict(data_pt, node.right)\n",
    "\n",
    "    def get_feature_threshold(self, node):\n",
    "        \"\"\" This function finds the feature that gives the largest information\n",
    "        gain, then updates node.threshold, node.threshold_index, and \n",
    "        node.feature, a number representing the feature.\n",
    "        return: None\n",
    "        \"\"\"\n",
    "        node.threshold = 0\n",
    "        node.threshold_index = 0\n",
    "        node.feature = 0\n",
    "\n",
    "        gain, index, feature = 0, 0, 0 \n",
    "        for k in range(len(node.data[0])):\n",
    "            d = np.argsort(node.data[:, k])\n",
    "            node.labels = node.labels[d]\n",
    "            node.data = node.data[d]\n",
    "            for j in range(len(node.data)):\n",
    "                if self.getInfoGain(node, j) > gain:\n",
    "                    gain, index, feature = self.getInfoGain(node, j), j, k\n",
    "        feat = np.argsort(node.data[:, feature])\n",
    "        node.data = node.data[feat]\n",
    "        node.labels = node.labels[feat]\n",
    "        \n",
    "        node.threshold = node.data[index, feature]\n",
    "        node.threshold_index = index\n",
    "        node.feature = feature \n",
    "\n",
    "    def getInfoGain(self, node, split_index):\n",
    "        \"\"\"\n",
    "        TODO Get information gain using the variables in the parameters,\n",
    "        split_index: index in the feature column that you are splitting the classes on\n",
    "        return: information gain (float)\n",
    "        \"\"\"\n",
    "        # TODO YOUR CODE HERE\n",
    "        left_uncertainty = self.get_uncertainty(node.labels[:split_index]) \n",
    "        right_uncertainty = self.get_uncertainty(node.labels[split_index:])\n",
    "        \n",
    "        n = len(node.labels)\n",
    "        w1 = left_uncertainty * (split_index+1) / n\n",
    "        w2 = right_uncertainty * (n-(split_index+1))/n\n",
    "        \n",
    "        start_entropy = self.get_uncertainty(node.labels)\n",
    "        conditional_entropy = w1 + w2 \n",
    "        infogain = start_entropy - conditional_entropy\n",
    "        return infogain\n",
    "\n",
    "    def get_uncertainty(self, labels, metric=\"gini\"):\n",
    "        \"\"\"\n",
    "        TODO Get uncertainty. Implement entropy AND gini index\n",
    "        metrics. np.bincount(labels) and labels.shape might be \n",
    "        useful here\n",
    "        return: uncertainty (float)\n",
    "        \"\"\"\n",
    "        \n",
    "        if labels.shape[0] == 0:\n",
    "            return 1\n",
    "        if metric ==\"gini\":    \n",
    "            uncertainty = gini(labels)\n",
    "        if metric == \"entropy\":\n",
    "            uncertainty = entropy(labels)\n",
    "        \n",
    "        return uncertainty\n",
    "\n",
    "    def printTree(self):\n",
    "        \"\"\"Prints the tree including threshold value and feature name\"\"\"\n",
    "        self._printTree(self.root)\n",
    "\n",
    "    def _printTree(self, node):\n",
    "        if node is not None:\n",
    "            if node.label is None:\n",
    "                print(\"\\t\" * node.depth, \"(%d, %d)\" % (node.threshold, node.feature))\n",
    "            else:\n",
    "                print(\"\\t\" * node.depth, node.label)\n",
    "            self._printTree(node.left)\n",
    "            self._printTree(node.right)\n",
    "\n",
    "    def tree_evaluate(self, X_train, labels, X_test, y_test):\n",
    "        n = X_train.shape[0]\n",
    "\n",
    "        count = 0\n",
    "        for i in range(n):\n",
    "            if self.predict(X_train[i]) == labels[i]:\n",
    "                count += 1\n",
    "\n",
    "        print(\"The decision tree is %d percent accurate on %d training data\" % ((count / n) * 100, n))\n",
    "\n",
    "        n = X_test.shape[0]\n",
    "\n",
    "        count = 0\n",
    "        for i in range(n):\n",
    "            if self.predict(X_test[i]) == y_test[i]:\n",
    "                count += 1\n",
    "\n",
    "        print(\"The decision tree is %d percent accurate on %d test data\" % ((count / n) * 100, n))\n",
    "\n",
    "        return count / n\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runnign the tree\n",
    "\n",
    "The depth of the tree, $K$, is the number of features that the tree will split on. Varying the value of $K$ affects the model accuracy, but there are diminshing returns as $K$ is increased more and more.\n",
    "\n",
    "Which feature gives the largest information gain? Which feature is the least useful for the decision tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root node shape:  (1098, 4) (1098,)\n",
      "Node uncertainty: 0.494610\n",
      "Node uncertainty: 0.309938\n",
      "Node uncertainty: 0.132653\n",
      "Node uncertainty: 0.074700\n",
      "Node uncertainty: 0.000000\n",
      "Node uncertainty: 0.221919\n",
      "Node uncertainty: 0.000000\n",
      "Node uncertainty: 0.117188\n",
      "Node uncertainty: 0.500000\n",
      "Node uncertainty: 0.000000\n",
      "Node uncertainty: 0.000000\n",
      "Node uncertainty: 0.000000\n",
      "Node uncertainty: 0.398023\n",
      "Node uncertainty: 0.052593\n",
      "Node uncertainty: 0.000000\n",
      "Node uncertainty: 0.000000\n",
      "Node uncertainty: 0.460800\n",
      "Node uncertainty: 0.320000\n",
      "Node uncertainty: 0.000000\n",
      "Node uncertainty: 0.000000\n",
      "Node uncertainty: 0.124444\n",
      "Node uncertainty: 0.000000\n",
      "Node uncertainty: 0.000000\n",
      "Node uncertainty: 0.277778\n",
      "Node uncertainty: 0.000000\n",
      "Node uncertainty: 0.000000\n",
      "Node uncertainty: 0.195083\n",
      "Node uncertainty: 0.440614\n",
      "Node uncertainty: 0.088697\n",
      "Node uncertainty: 0.000000\n",
      "Node uncertainty: 0.000000\n",
      "Node uncertainty: 0.221542\n",
      "Node uncertainty: 0.019998\n",
      "Node uncertainty: 0.000000\n",
      "Node uncertainty: 0.066587\n",
      "Node uncertainty: 0.000000\n",
      "Node uncertainty: 0.000000\n",
      "Node uncertainty: 0.496327\n",
      "Node uncertainty: 0.000000\n",
      "Node uncertainty: 0.235537\n",
      "Node uncertainty: 0.000000\n",
      "Node uncertainty: 0.000000\n",
      "Node uncertainty: 0.020354\n",
      "Node uncertainty: 0.000000\n",
      "Node uncertainty: 0.015344\n",
      "Node uncertainty: 0.117188\n",
      "Node uncertainty: 0.375000\n",
      "Node uncertainty: 0.000000\n",
      "Node uncertainty: 0.000000\n",
      "Node uncertainty: 0.000000\n",
      "Node uncertainty: 0.000000\n",
      "The decision tree is 100 percent accurate on 1098 training data\n",
      "The decision tree is 96 percent accurate on 274 test data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9671532846715328"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = import_data(split=0.8, shuffle=True)\n",
    "\n",
    "tree = DecisionTree(K=7, verbose=True)\n",
    "tree.buildTree(X_train, y_train)\n",
    "\n",
    "tree.tree_evaluate(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K=0\n",
      "The decision tree is 85 percent accurate on 1098 training data\n",
      "The decision tree is 83 percent accurate on 274 test data\n",
      "\n",
      "\n",
      "K=1\n",
      "The decision tree is 91 percent accurate on 1098 training data\n",
      "The decision tree is 91 percent accurate on 274 test data\n",
      "\n",
      "\n",
      "K=2\n",
      "The decision tree is 94 percent accurate on 1098 training data\n",
      "The decision tree is 93 percent accurate on 274 test data\n",
      "\n",
      "\n",
      "K=3\n",
      "The decision tree is 96 percent accurate on 1098 training data\n",
      "The decision tree is 93 percent accurate on 274 test data\n",
      "\n",
      "\n",
      "K=4\n",
      "The decision tree is 98 percent accurate on 1098 training data\n",
      "The decision tree is 97 percent accurate on 274 test data\n",
      "\n",
      "\n",
      "K=5\n",
      "The decision tree is 99 percent accurate on 1098 training data\n",
      "The decision tree is 97 percent accurate on 274 test data\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in np.arange(6):\n",
    "    X_train, y_train, X_test, y_test = import_data(split=0.8, shuffle=True)\n",
    "    tree = DecisionTree(K=i, verbose=False)\n",
    "    print('K=' + str(i))\n",
    "    tree.buildTree(X_train, y_train)\n",
    "    tree.tree_evaluate(X_train, y_train, X_test, y_test)\n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "tensorflow_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
